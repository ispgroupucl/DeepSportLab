{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seven-costa",
   "metadata": {},
   "source": [
    "# ROC on openpifpaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "class OutputInhibitor():\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "    def __enter__(self):\n",
    "        if self.name:\n",
    "            print(\"Launching {}... \".format(self.name), end=\"\")\n",
    "        self.ps1, self.ps2 = getattr(sys, \"ps1\", None), getattr(sys, \"ps2\", None)\n",
    "        if self.ps1:\n",
    "            del sys.ps1\n",
    "        if self.ps2:\n",
    "            del sys.ps2\n",
    "        self.stderr = sys.stderr\n",
    "        self.fp = open(os.devnull, \"w\")\n",
    "        sys.stderr = self.fp\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.ps1:\n",
    "            sys.ps1 = self.ps1\n",
    "        if self.ps2:\n",
    "            sys.ps2 = self.ps2\n",
    "        sys.stderr = self.stderr\n",
    "        self.fp.close()\n",
    "        if self.name:\n",
    "            print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-repository",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mlworkflow import PickledDataset, TransformedDataset\n",
    "from dataset_utilities.ds.instants_dataset import ExtractViewData, ViewCropperTransform\n",
    "with OutputInhibitor():\n",
    "    from openpifpaf.datasets.deepsport import AddBallSegmentationTargetViewFactory, AddBallPositionFactory\n",
    "\n",
    "ds = PickledDataset(\"/scratch/gva/views_camera_with_ball2.pickle\")\n",
    "\n",
    "shape = (800,600)\n",
    "\n",
    "ds = TransformedDataset(ds, [\n",
    "    ViewCropperTransform(def_min=30, def_max=80, output_shape=shape),\n",
    "    ExtractViewData(AddBallPositionFactory(), AddBallSegmentationTargetViewFactory()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "keys = list(PickledDataset(\"/scratch/gva/camera_views_with_human_masks.pickle\").keys.all())\n",
    "random.seed(0)\n",
    "random.shuffle(keys)\n",
    "validation_set_size_pc = 15\n",
    "lim = len(keys)*validation_set_size_pc//100\n",
    "training_keys = keys[lim:]\n",
    "validation_keys = keys[:lim]\n",
    "testing_keys = [k for k in ds.yield_keys() if k not in training_keys and k not in validation_keys]\n",
    "print(len(validation_keys))\n",
    "print(len(training_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_layers import AvoidLocalEqualities, PeakLocalMax, ComputeElementaryMetrics\n",
    "class ChunkProcessor:\n",
    "    pass\n",
    "class CastFloat(ChunkProcessor):\n",
    "    def __init__(self, tensor_name):\n",
    "        self.tensor_name = [tensor_name] if isinstance(tensor_name, str) else tensor_name\n",
    "    def __call__(self, chunk):\n",
    "        for tensor_name in self.tensor_name:\n",
    "            if tensor_name in chunk:\n",
    "                chunk[tensor_name] = tf.cast(chunk[tensor_name], tf.float32)\n",
    "class Normalize(ChunkProcessor):\n",
    "    def __init__(self, tensor_name):\n",
    "        self.tensor_name = tensor_name\n",
    "    def __call__(self, chunk):\n",
    "        assert chunk[self.tensor_name].dtype == tf.float32\n",
    "        chunk[self.tensor_name] = chunk[self.tensor_name]/255\n",
    "class ComputeKeypointsDetectionAccuracy(ChunkProcessor):\n",
    "    def __init__(self, non_max_suppression_pool_size=50, threshold=0.5, target_enlargment_size=10):\n",
    "        thresholds = threshold if isinstance(threshold, np.ndarray) else np.array([threshold])\n",
    "        assert len(thresholds.shape) == 1, \"'threshold' argument should be 1D-array (a scalar is also accepted).\"\n",
    "\n",
    "        self.avoid_local_eq = AvoidLocalEqualities()\n",
    "        self.peak_local_max = PeakLocalMax(min_distance=non_max_suppression_pool_size//2, thresholds=thresholds)\n",
    "        self.enlarge_target = tf.keras.layers.MaxPool2D(target_enlargment_size, strides=1, padding=\"same\")\n",
    "        self.compute_metric = ComputeElementaryMetrics()\n",
    "\n",
    "    def __call__(self, chunk):\n",
    "        batch_target = tf.cast(chunk[\"batch_target\"], tf.float32)\n",
    "        batch_target = batch_target if len(batch_target.shape) == 4 else batch_target[...,tf.newaxis]\n",
    "        batch_output = chunk[\"batch_heatmap\"]\n",
    "        batch_output = batch_output if len(batch_output.shape) == 4 else batch_output[...,tf.newaxis]\n",
    "\n",
    "        batch_output = self.avoid_local_eq(batch_output)\n",
    "        batch_hitmap = self.peak_local_max(batch_output)\n",
    "        batch_hitmap = tf.cast(batch_hitmap, tf.int32)\n",
    "        chunk[\"batch_hitmap\"] = tf.squeeze(batch_hitmap)\n",
    "        batch_target = self.enlarge_target(batch_target)\n",
    "        batch_target = tf.cast(batch_target, tf.int32)[..., tf.newaxis]\n",
    "\n",
    "        batch_metric = self.compute_metric(batch_hitmap=batch_hitmap, batch_target=batch_target)\n",
    "        chunk[\"batch_TP\"] = tf.squeeze(batch_metric[\"batch_TP\"])\n",
    "        chunk[\"batch_FP\"] = tf.squeeze(batch_metric[\"batch_FP\"])\n",
    "        chunk[\"batch_TN\"] = tf.squeeze(batch_metric[\"batch_TN\"])\n",
    "        chunk[\"batch_FN\"] = tf.squeeze(batch_metric[\"batch_FN\"])\n",
    "\n",
    "        \n",
    "chunk = {}\n",
    "chunk[\"batch_heatmap\"] = tf.keras.Input(dtype=tf.uint8, shape=(shape[1], shape[0]), name=\"batch_heatmap\")\n",
    "chunk[\"batch_target\"] = tf.keras.Input(dtype=tf.uint8, shape=(shape[1], shape[0]), name=\"batch_target\")\n",
    "inputs = dict(chunk) # makes a copy\n",
    "\n",
    "thresholds = np.array([])\n",
    "n_points = 21\n",
    "chunk_processors = [\n",
    "    CastFloat([\"batch_heatmap\", \"batch_target\"]),\n",
    "    Normalize(\"batch_heatmap\"),\n",
    "    ComputeKeypointsDetectionAccuracy(non_max_suppression_pool_size=20, threshold=np.linspace(0,1,n_points)),\n",
    "]\n",
    "for cp in chunk_processors:\n",
    "    cp(chunk)\n",
    "\n",
    "outputs = {k:chunk[k] for k in chunk if k in [\"batch_TP\", \"batch_TN\", \"batch_FP\", \"batch_FN\"]}\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot(current_result, set_name, fig=None):\n",
    "    fig = fig or plt.figure()\n",
    "    x = current_result[\"FP\"]/(current_result[\"FP\"]+current_result[\"TN\"])\n",
    "    y = current_result[\"TP\"]/(current_result[\"TP\"]+current_result[\"FN\"])\n",
    "    ax = fig.gca()\n",
    "    ax.plot(x, y, linestyle=\"-\", linewidth=1, markersize=5, marker=\".\", label=set_name)\n",
    "    index=10\n",
    "    ax.plot(x[index], y[index], markersize=10, marker=\".\", color=\"green\")\n",
    "    ax.set_xlabel(\"FP rate\")\n",
    "    ax.set_ylabel(\"TP rate\")\n",
    "    ax.axis(\"equal\")\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_box_aspect(1)\n",
    "    ax.set_title(f\"ROC ball detection from PIFPAF\")\n",
    "    ax.legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from ipywidgets import Output\n",
    "\n",
    "output = Output()\n",
    "display.display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(weight_file, data):\n",
    "    image_filename = \"/home/gva/tmp_image.png\"\n",
    "    imageio.imwrite(image_filename, data[\"input_image\"])\n",
    "    sys.argv = [\n",
    "        \"aue\",\n",
    "        image_filename,\n",
    "        \"--checkpoint\", weights_file,\n",
    "        \"--image-output\",\n",
    "        \"--debug-images\", \"--debug-cif-c\", \"--debug\"\n",
    "    ]\n",
    "    with OutputInhibitor():\n",
    "        main()\n",
    "    return imageio.imread(\"image/test.accumulated.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(ds.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.query_item(next(it))\n",
    "hm = infer(\"outputs/resnet50-210311-144027.099429-ball-edge641.pkl.epoch100\", data)\n",
    "plt.imshow(data[\"input_image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-suicide",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from openpifpaf.predict import main\n",
    "import sys\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#weights_file = \"shufflenetv2k16w-210225-164102-ball-edge501.pkl.epoch150\" # trained on small dataset\n",
    "#weights_file = \"shufflenetv2k16w-210308-164747-ball-edge501.pkl.epoch150\" # trained on full dataset\n",
    "#weights_file = \"shufflenetv2k16-210309-153430-ball-edge501.pkl.epoch200\" # trained on small dataset with fixed seed\n",
    "#weights_file = \"shufflenetv2k16-210309-220522-ball.pkl.epoch499\" # trained on small dataset with fixed seed\n",
    "\n",
    "import glob\n",
    "#for filename in glob.glob(\"outputs/resnet*.log\"):\n",
    "#for filename in glob.glob(\"outputs/*210312-0[89]*.log\"):\n",
    "for filename in glob.glob(\"outputs/*210312-1*.log\"):\n",
    "    #weights_file = filename.replace(\".log\", \".epoch100\")\n",
    "    weights_file = glob.glob(filename.replace(\".log\", \".epoch*\"))[-1]\n",
    "    #weights_file = \"results/shufflenetv2k16w-210308-164747-ball-edge501.pkl.epoch150\"\n",
    "    if not os.path.isfile(weights_file):\n",
    "        #raise\n",
    "        continue\n",
    "    \n",
    "    sets_results = {}\n",
    "    for set_name in [\"training_keys\", \"validation_keys\"]: # \"testing_keys\"\n",
    "        current_result = sets_results[set_name] = {}\n",
    "        current_result[\"TP\"] = np.zeros(n_points, np.int32)\n",
    "        current_result[\"FP\"] = np.zeros(n_points, np.int32)\n",
    "        current_result[\"TN\"] = np.zeros(n_points, np.int32)\n",
    "        current_result[\"FN\"] = np.zeros(n_points, np.int32)\n",
    "        for key in tqdm(eval(set_name)):\n",
    "            data = ds.query_item(key)\n",
    "            if data is None:\n",
    "                continue\n",
    "            \n",
    "            heatmap = infer(weights_file, data)\n",
    "            \n",
    "            result = model({\"batch_heatmap\": heatmap[np.newaxis], \"batch_target\": data[\"mask\"][np.newaxis]})\n",
    "            current_result[\"TP\"] += result[\"batch_TP\"].numpy()\n",
    "            current_result[\"FP\"] += result[\"batch_FP\"].numpy()\n",
    "            current_result[\"TN\"] += result[\"batch_TN\"].numpy()\n",
    "            current_result[\"FN\"] += result[\"batch_FN\"].numpy()\n",
    "            with output:\n",
    "                display.clear_output(wait=True)\n",
    "                fig = plot(current_result, set_name)\n",
    "                display.display(fig)\n",
    "    sets_results[\"filename\"] = filename\n",
    "    pickle.dump(sets_results, open(filename+\".pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-going",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for file in glob.glob(\"outputs/*210312-*.pickle\"):\n",
    "#for file in glob.glob(\"outputs/*.pickle\"):\n",
    "    print(file)\n",
    "    d = pickle.load(open(file, \"rb\"))\n",
    "    fig = None\n",
    "    for name in [\"training_keys\", \"validation_keys\"]:\n",
    "        fig = plot(d[name], name, fig)\n",
    "        fig.gca().set_title(file)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sets_results, open(filename+\".pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
